{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSnUZE4wVJ05",
    "outputId": "a4317719-7de9-407f-9ceb-e152000f5f71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1s64DpxbfRPAjmFdx14ji4RkKPU8KlQge\n",
      "To: /content/data/images_1.5M_250x500.tar.gz\n",
      "3.01GB [01:00, 49.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install gdown\n",
    "rm -rf *\n",
    "mkdir images\n",
    "mkdir data\n",
    "cd data\n",
    "gdown 'https://drive.google.com/uc?id=1s64DpxbfRPAjmFdx14ji4RkKPU8KlQge'\n",
    "tar -xvf images_1.5M_250x500.tar.gz\n",
    "rm -rf images_1.5M_250x500.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Qeg8I6wQDzQ0"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "import gc\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 250\n",
    "        self.img_cols = 500\n",
    "        self.channels = 1\n",
    "        self.img_shape = (124, 248, 1)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(layers.Dense(31*62*128, use_bias=False, input_dim=self.latent_dim))\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.LeakyReLU())\n",
    "\n",
    "      model.add(layers.Reshape((31, 62, 128)))\n",
    "      assert model.output_shape == (None,31, 62, 128)\n",
    "\n",
    "      model.add(layers.Conv2DTranspose(128, (5, 10), strides=(1, 1), padding='same', use_bias=False))\n",
    "      print(model.output_shape)\n",
    "      assert model.output_shape == (None, 31, 62, 128)\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.LeakyReLU())\n",
    "\n",
    "      model.add(layers.Conv2DTranspose(64, (5, 10), strides=(2, 2), padding='same', use_bias=False))\n",
    "      print(model.output_shape)\n",
    "      assert model.output_shape == (None, 62, 124, 64)\n",
    "      model.add(layers.BatchNormalization())\n",
    "      model.add(layers.LeakyReLU())\n",
    "\n",
    "      model.add(layers.Conv2DTranspose(1, (5, 10), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "      print(model.output_shape)\n",
    "      assert model.output_shape == (None, 124, 248, 1)\n",
    "\n",
    "\n",
    "      noise = Input(shape=(self.latent_dim,))\n",
    "      img = model(noise)\n",
    "\n",
    "      return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same',\n",
    "                                      input_shape=self.img_shape))\n",
    "      model.add(layers.LeakyReLU())\n",
    "      model.add(layers.Dropout(0.3))\n",
    "\n",
    "      model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "      model.add(layers.LeakyReLU())\n",
    "      model.add(layers.Dropout(0.3))\n",
    "\n",
    "      model.add(layers.Flatten())\n",
    "      model.add(layers.Dense(1))\n",
    "\n",
    "      img = Input(shape=self.img_shape)\n",
    "      validity = model(img)\n",
    "\n",
    "      return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        train_it = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "          directory='data', target_size=(124, 248),\n",
    "          color_mode='grayscale',\n",
    "          batch_size=batch_size,\n",
    "          image_data_generator=ImageDataGenerator(),\n",
    "          shuffle=True\n",
    "        )\n",
    "\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            X_train,_ = train_it.next()\n",
    "            if X_train.shape[0] < batch_size:\n",
    "              train_it = tf.keras.preprocessing.image.DirectoryIterator(\n",
    "                directory='data', target_size=(248, 496),\n",
    "                color_mode='grayscale',\n",
    "                batch_size=batch_size,\n",
    "                image_data_generator=ImageDataGenerator(),\n",
    "                shuffle=True\n",
    "              )\n",
    "              X_train,_ = train_it.next()\n",
    "\n",
    "            X_train = X_train / 127.5 - 1.\n",
    "            X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(X_train, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "            if epoch % 3000 == 0 and epoch > 0:\n",
    "                self.generator.save_weights('dados_generator%d.h5' % (epoch))\n",
    "                self.discriminator.save_weights('dados_discriminator%d.h5' % (epoch))\n",
    "                gc.collect()\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def new_sample(self):\n",
    "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
    "        return tf.reshape(self.generator.predict(noise)[0], [250,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pMda5HzUIWJ3",
    "outputId": "9abc6758-f92a-447a-882e-5f285355b9fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 31, 62, 128)\n",
      "(None, 62, 124, 64)\n",
      "(None, 124, 248, 1)\n",
      "Found 1584663 images belonging to 1 classes.\n",
      "0 [D loss: 2.652147, acc.: 50.00%] [G loss: 0.001643]\n",
      "1 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000000]\n",
      "2 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "3 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "4 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.000000]\n",
      "5 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "6 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "7 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "8 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "9 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "10 [D loss: 0.002127, acc.: 100.00%] [G loss: 0.000000]\n",
      "11 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.031364]\n",
      "12 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "13 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "14 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "15 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "16 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "17 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "18 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "19 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "20 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "21 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "22 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.000532]\n",
      "23 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.003068]\n",
      "24 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "25 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000000]\n",
      "26 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.352033]\n",
      "27 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.211440]\n",
      "28 [D loss: 0.150580, acc.: 100.00%] [G loss: 15.424949]\n",
      "29 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.424949]\n",
      "30 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "31 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "32 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "33 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "34 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "35 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "36 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "37 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "38 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "39 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "40 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "41 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "42 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "43 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "44 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "45 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "46 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "47 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "48 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "49 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "50 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "51 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "52 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "53 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "54 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "55 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "56 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "57 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "58 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "59 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "60 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "61 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "62 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "63 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "64 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "65 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "66 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "67 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "68 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "69 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "70 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "71 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "72 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "73 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "74 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "75 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "76 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "77 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "78 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "79 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "80 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "81 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "82 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "83 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "84 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "85 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "86 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "87 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "88 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "89 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "90 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "91 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "92 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "93 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "94 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "95 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "96 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "97 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "98 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "99 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "100 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "101 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "102 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "103 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "104 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "105 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "106 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "107 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "108 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "109 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "110 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "111 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "112 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "113 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "114 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "115 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "116 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "117 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "118 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "119 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "120 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "121 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "122 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "123 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "124 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "125 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "126 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "127 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "128 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "129 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "130 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "131 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "132 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "133 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "134 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "135 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "136 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "137 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "138 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n",
      "139 [D loss: 7.712474, acc.: 50.00%] [G loss: 15.424949]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-60dd13741a47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bb9d9cde1bae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \"\"\"\n\u001b[1;32m   1668\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   def train_on_batch(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3706\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3707\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    891\u001b[0m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[1;32m    892\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m--> 893\u001b[0;31m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[1;32m    894\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 142\u001b[0;31m         _ctx, \"AssignVariableOp\", name, resource, value)\n\u001b[0m\u001b[1;32m    143\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GAN().train(epochs=10000, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DCGan.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
